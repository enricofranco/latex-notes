\chapter{Synchronization}
In concurrent programming, where processes or threads cooperate and share data, race conditions may arise and there may be sections of non-reentrant code, it is needed to properly synchronize the cooperating actors to make their results not dependent on their relative speed.

A \emph{Critical Section} or \emph{Critical Region} is a section of code, common to multiple threads, in which they can access shared objects or in which they are competing for the use of shared resources. It is needed to ensure that when one thread is executing in its critical section, no other thread is allowed to execute in its critical section, therefore an access protocol must be established to enter the critical section in \textbf{mutual exclusion}:

\begin{itemize}
\item A thread executes a ``reservation'' code before entering a CS;
\item The reservation code blocks the thread if another is using its CS;
\item Leaving its CS, a thread executes a code to release the CS;
\item The release possibly unlocks another thread which was waiting in the ``reservation'' code of its CS.
\end{itemize}

In general, \textbf{software solutions} to the problem of critical section are complex and inefficient. In fact, test and set operations are ``invisible'' to other threads and they are not atomic, thus a thread can react to the presumed value of a variable rather than to its current value. Furthermore, the solutions is not easily extensible to an arbitrary number of threads.

Instead, \textbf{hardware solutions} can be easily extensible to a larger number of threads but they introduce busy form of waiting with spin lock, leading to possible starvation.

\section{Semaphores}
Hardware solutions can be used to implement system calls useful to solve every kind of synchronization problem (not only the Mutual Exclusion) avoiding the busy form of waiting. These system calls rely on a data structure introduce by Dijkstra in 1965 called semaphore.

A \emph{semaphore} is a shared data structure including a counter and a waiting queue managed by the kernel, both protected by a lock.
Kernel offers a set of \textbf{atomic primitives} that allows a thread to be blocked on the semaphore or to wake up if it was locked.

\begin{description}
\item \texttt{init(S, k)} defines and initializes semaphore \texttt{S} counter to value \texttt{k}. \texttt{k} is a positive value because the system calls manage the counter so that, if negative, its absolute value is the number of threads waiting on the semaphore queue.
\item \texttt{wait(S)} decrements the counter and blocks the calling thread if the counter value of \texttt{S} is negative or zero. Originally called \texttt{p} operation.
\item \texttt{signal(S)} increases the counter and makes some blocked threads ready to run if the counter value of \texttt{S} is negative or zero. Originally called \texttt{v} operation.
\item \texttt{destroy(S)} releases semaphore \texttt{S} memory.
\end{description}

Semaphores can be used to solve any synchronization problem using an appropriate protocol, possibly adding more than one semaphore and additional shared variables.

\subsection{POSIX semaphores}
POSIX defines a set of kernel independent system calls to manage semaphores, included in header file \texttt{semaphore.h} which return -1 on error.

\begin{description}
\item \texttt{int sem\_init(sem\_t *sem, int pshared, usigned int value)} initializes the semaphore counter at value \texttt{value}. The \texttt{pshared} value identifies the type of semaphore. If it is equal to 0, the semaphore is local to the threads of current process, otherwise the semaphore can be shared between different processes.

\item \texttt{int sem\_wait(sem\_t *sem)} implements the standard wait, i.e.,\@ if the counter is negative or zero, the calling thread is blocked.

\item \texttt{int sem\_post(sem\_t *sem)} implements the standard signal, i.e.,\@ increments the counter and wakes up a blocking thread if the counter is negative or zero.

\item \texttt{int sem\_getvalue(sem\_t *sem, int *valP)} allows obtaining the value of the semaphore counter. The value is assigned to \texttt{*valP}.

\item \texttt{int sem\_destroy(sem\_t *sem)} destroys the semaphore pointed by \texttt{sem}.

\item \texttt{int sem\_trywait(sem\_t *sem)} implements a non-blocking wait, i.e.,\@ if the semaphore counter has a value greater than 0, it performs the decrement and returns 0 while, if the semaphore counter is negative or zero, it returns -1 instead of blocking the caller as \texttt{sem\_wait} does.
\end{description}

\subsection{Pthread mutex}
Pthread mutex defines binary semaphores of type \texttt{pthread\_mutex\_t} by means of system calls
\begin{description}
\item \texttt{int pthread\_mutex\_init(pthread\_mutex\_t *mutex, \newline const pthread\_mutexattr\_t *attr)} initializes the mutex referenced by \texttt{mutex} with attributes specified by \texttt{attr} (default = \texttt{NULL}).

\item \texttt{int pthread\_mutex\_lock(pthread\_mutex\_t *mutex)} blocks the caller if the \texttt{mutex} is locked or acquires the mutex lock if the \texttt{mutex} is unlocked.

\item \texttt{int pthread\_mutex\_trylock(pthread\_mutex\_t *mutex)} is similar to \texttt{lock}, but it returns without blocking the caller if \texttt{mutex} is locked. It returns 0 if the lock has been successfully acquired or a \texttt{EBUSY} error if  the \texttt{mutex} was already locked.

\item \texttt{int pthread\_mutex\_unlock(pthread\_mutex\_t *mutex)} releases the \texttt{mutex} lock.

\item \texttt{int pthread\_mutex\_destroy(pthread\_mutex\_t *mutex)} frees \texttt{mutex} memory so that it cannot be used anymore.
\end{description}

\section{Synchronization protocols with semaphores}
\subsection{Producer \& Consumer with limited memory buffer}
This problem uses a circular buffer implementing a FIFO queue of size \texttt{MAX} for storing the produced elements to be consumed. If the buffer is empty, the consumer has to wait. On the other hand, if the buffer is full, the producer has to wait. In an intermediate situation, producer and consumer can produce and consume in a concurrent fashion.

\begin{figure}[hbtp]
\centering
\includegraphics[scale=0.4]{images/synchronization/producer_consumer_buffer.png}
\caption{Circular buffer}
\end{figure}

\paragraph{Access functions}
Functions without considering synchronization and concurrency.

\begin{Parallel}{}{}
\ParallelLText{
\begin{verbatim}
void enqueue(int val) {
  queue[in] = val;
  in = (in + 1) % MAX;
  return;
}
\end{verbatim}}
\ParallelRText{
\begin{verbatim}
void dequeue(int *val) {
  queue[out] = val;
  out = (out + 1) % MAX;
  return;
}
\end{verbatim}}
\end{Parallel}

\paragraph{Concurrent access}
Initially, a consumer has to wait and a producer has to produce, i.e.,\@ it has not to be blocked. Therefore, a consumer has to wait on full, initialized to 0 and a producer has to produce at most \texttt{MAX} elements. Both signal when an elements is enqueued or dequeued.
\begin{verbatim}
init(full, 0);
init(empty, MAX);
\end{verbatim}

\begin{Parallel}{}{}
\ParallelLText{
\begin{verbatim}
Producer() {
  Message m;
  while(TRUE) {
    produce(m);
    wait(empty);
    enqueue(m);
    signal(full);
  }
}
\end{verbatim}}
\ParallelRText{
\begin{verbatim}
Consumer() {
  Message m;
  while(TRUE) {
    wait(full);
    m = dequeue();
    signal(empty);
    consume(m);
  }
}
\end{verbatim}}
\end{Parallel}

Producer and consumer operate on different indexes of the buffer, thus they can operate in concurrency as long as the queue is not full or empty, otherwise either a producer or a consumer is blocked.

The solution is symmetric and it can be easily extended to more than one producer and consumer processes, remembering that operations on the queue are not atomic and multiple consumers or multiple producers must act in mutual exclusion.

\paragraph{Producers \& Consumers}
\begin{verbatim}
init(full, 0);
init(empty, MAX);
init(meP, 1);
init(meC, 1);
\end{verbatim}

\begin{Parallel}{}{}
\ParallelLText{
\begin{verbatim}
Producer() {
  Message m;
  while(TRUE) {
    produce(m);
    wait(empty);
    wait(meP);
    enqueue(m);
    signal(meP);
    signal(full);
  }
}
\end{verbatim}}
\ParallelRText{
\begin{verbatim}
Consumer() {
  Message m;
  while(TRUE) {
    wait(full);
    wait(meC);
    m = dequeue();
    signal(meC);
    signal(empty);
    consume(m);
  }
}
\end{verbatim}}
\end{Parallel}

\newpage

\subsection{Readers \& Writers}
\emph{Readers \& Writers} represents the typical problem of sharing a database between two sets of concurrent threads:
\begin{itemize}
\item Readers are allowed to access the database in concurrency;
\item Writers must access the database in mutual exclusion with other Writers and Readers.
\end{itemize}
When a writer is writing in the database, several Readers and Writers processes can be blocked waiting the end of the write operation.

\begin{itemize}
\item \textbf{Readers precedence}: at the end of a writing operation, favour the access of the waiting Readers rather than of the waiting Writers.
\item \textbf{Writers precedence}: at the end of a writing operation, favour the access of the waiting Writers rather than of the waiting Readers.
\end{itemize}

\paragraph{Readers priority}
Giving priority to the Readers means that a Reader does not wait unless a Writer is writing. While Readers are reading, new Readers are allowed to read and Writers are blocked. The first Reader blocks any Writer and, dually, when the last Reader terminates, a waiting Writer can access the database. The solution uses:

\begin{itemize}
\item A shared variable \texttt{nR} that counts the number of Readers inside the critical section;
\item A mutual exclusion semaphore \texttt{meR} to protect variable \texttt{nR};
\item A mutual exclusion semaphore \texttt{w} among Writers or among Readers and Writers;
\item A mutual exclusion semaphore \texttt{meW} among Writers.
\end{itemize}

\begin{verbatim}
nR = 0;
init(meR, 1);
init(meW, 1);
init(w, 1);
\end{verbatim}

\begin{Parallel}{}{}
\ParallelLText{
\begin{verbatim}
Reader() {
  wait(meR);
    nR++;
    if(nR == 1)
      wait(w);
  signal(meR);
  // read
  wait(meR);
    nR--;
    if(nR == 0)
      signal(w);
  signal(meR);
}
\end{verbatim}}
\ParallelRText{
\begin{verbatim}
Writer() {
  wait(meW);
  wait(w);
  // write
  signal(w);
  signal(meW);
}
\end{verbatim}}
\end{Parallel}

\paragraph{Writers priority}
Giving priority to the Writers means that a Writer has priority over all Readers. A Writer trying to enter its critical section blocks \textit{new} Readers, but the Readers that are inside their critical section are allowed to complete their reading tasks. The solution uses:

\begin{itemize}
\item Two shared variables \texttt{nR} and \texttt{nW} to count the Readers inside the critical section and the Writers that need to write (one of them possibly writing);
\item Two mutual exclusion semaphores \texttt{meR} and \texttt{meW} to protect the variables \texttt{nR} and \texttt{nW};
\item Two mutual exclusion semaphores \texttt{r} and \texttt{w} to enforce Readers and Writers to wait on different queues.
\end{itemize}

\begin{verbatim}
nR = nW = 0;
init(meR, 1);
init(meW, 1);
init(w, 1);
init(r, 1);
\end{verbatim}

\begin{Parallel}{}{}
\ParallelLText{
\begin{verbatim}
Reader() {
  wait(r);
    wait(meR);
      nR++;
      if(nR == 1)
        wait(w);
    signal(meR);
  signal(r);
  // read
  wait(meR);
    nR--;
    if(nR == 0)
      signal(w);
  signal(meR);
}
\end{verbatim}}
\ParallelRText{
\begin{verbatim}
Writer() {
  wait(meW);
    nW++;
    if(nW == 1)
      wait(r);
  signal(meW);
  wait(w);
  // write
  signal(w);
  wait(meW);
    nW--;
    if(nW == 0)
      signal(r);
  signal(meW);
}
\end{verbatim}}
\end{Parallel}

\subsection{Single lane tunnel}
A tunnel has a single lane, and cars can proceed only in alternate directions. Therefore it is needed to enable any number of cars (threads) to proceed in the same direction and block traffic in one direction if there is traffic in the opposite one.

\begin{figure}[hbtp]
\centering
\includegraphics[scale=0.4]{images/synchronization/single_lane_tunnel.png}
\caption{Single lane tunnel}
\end{figure}

This problem is similar the Readers \& Writers one, but for two sets of Readers. In its basic implementation can result in starvation of cars in one direction. The solution is perfectly symmetric and uses:
\begin{itemize}
\item Two shared count variables \texttt{n1} and \texttt{n2}, one for each travel direction;
\item Two semaphores \texttt{s1} and \texttt{s2}, one for each travel direction;
\item A global semaphore wait \texttt{busy}.
\end{itemize}

\begin{verbatim}
n1 = n2 = 0;
init(s1, 1);
init(s2, 1);
init(busy, 1);
\end{verbatim}

\begin{Parallel}{}{}
\ParallelLText{
\begin{verbatim}
left2right() {
  wait(s1);
    n1++;
    if(n1 == 1)
      wait(busy);
  signal(s1);
  // run left to right
  wait(s1);
    n1--;
    if(n1 == 0)
      signal(busy);
  signal(s1);
}
\end{verbatim}}
\ParallelRText{
\begin{verbatim}
right2left() {
  wait(s2);
    n2++;
    if(n2 == 1)
      wait(busy);
  signal(s2);
  // run right to left
  wait(s2);
    n2--;
    if(n" == 0)
      signal(busy);
  signal(s2);
}
\end{verbatim}}
\end{Parallel}

\subsection{Conditions}
A \emph{condition} is a data structure containing a mutual exclusion lock, a condition variable and a data structure requiring synchronization to protect.

\begin{verbatim}
typedef struct Cond {
  pthread_mutex_t lock;
  pthread_cond_t mycond;
  int i;
} Cond;
\end{verbatim}

\begin{description}
\item \texttt{lock} protects the structure;
\item \texttt{mycond} is the condition variable;
\item \texttt{i} is the variable to set or check.
\end{description}

The main thread has to define such a structure, allocate a global structure of that type, initialize its variables and create two threads A and B.

\textit{Thread~A} does work until it needs to check a given condition. It locks the associated mutex and checks the value of a global variable. By calling \texttt{pthread\_cond\_wait}, it performs a non-blocking wait for a signal from Thread~B and automatically unlocks the mutex lock, so that it can be used by Thread~B. When signaled, Thread~A wakes up with mutex lock automatically locked and, after explicitly unlocking the mutex lock, it can continues its work.

\textit{Thread~B} does work and locks the condition lock. It changes the value of the global variable associated to the waiting condition and, if Thread~A waiting condition is true, it performs a \texttt{pthread\_cond\_signal}. After unlocking the condition lock, it can continue its work.

\paragraph{Waiting on Condition Variables}
\begin{description}
\item \texttt{int pthread\_cond\_wait(pthread\_cond\_t *cond, \newline pthread\_mutex\_t *mutex)} blocks the calling thread until the specified condition is signaled. This system call should be called while \texttt{mutex} is locked, and it will automatically release the \texttt{mutex} while it waits.

Once signal is received and thread is awakened, \texttt{mutex} will be automatically locked for use by the thread.

The programmer is then responsible for unlocking \texttt{mutex} when the thread does not more need it.
\end{description}

\paragraph{Signaling on Condition Variables}
\begin{description}
\item \texttt{int pthread\_cond\_signal(pthread\_cond\_t *cond)} signals another thread which is waiting on the condition variable.

It should be called with the \texttt{mutex} locked, and must unlock it to allow \texttt{pthread\_cond\_wait} to complete.

\item \texttt{int pthread\_cond\_broadcast(pthread\_cond\_t *cond)} should be used instead of \texttt{pthread\_cond\_signal} if more than one thread is blocked on the same condition variable.
\end{description}
Calling \texttt{pthread\_cond\_signal} before \texttt{pthread\_cond\_wait} is a logical error.

\subsection{Precedence graphs}
Goal is to implement a graph using the minimum number of semaphores remembering that signals are sent to a semaphore, not to a process and they are caught by the first process able to receive them.

The first thing to do is to delete arcs which are not necessary and can, therefore, lead to use a non minimum number of semaphores.

\textbf{Acyclic threads}: outgoing arcs require a single semaphore and incoming arcs require a single semaphore.

\textbf{Cyclic threads}: outgoing arcs require different semaphores, while incoming arcs use a single  semaphore. In fact, using the same semaphore, because of the loop statement, a single process can get two signals if it is very fast.

\subsection{Path expressions}
\paragraph{Pure synchronization}
Pure synchronization is showm on figure~\ref{img:puresync}.

\underline{path} a ; b \underline{end}
\begin{verbatim}
init(s0) = 1, init(s1) = 0
wait(s0) a; b signal (s0)
A: wait(s0) a signal(s1)
B: wait(s1) b signal(s0)
\end{verbatim}

\begin{figure}[hbtp]
\centering
\includegraphics[scale=0.6]{images/synchronization/pathexp_puresynchronization.png}
\caption{Pure synchronization}
\label{img:puresync}
\end{figure}

\paragraph{Mutual exclusion}
Mutual exclusion is shows on figure~\ref{img:mutex}.

\underline{path} a + b \underline{end}
\begin{verbatim}
init(s0) = 1
A: wait(s0) a signal(s0)
B: wait(s0) b signal(s0)
\end{verbatim}

\begin{figure}[hbtp]
\centering
\includegraphics[scale=0.6]{images/synchronization/pathexp_mutex.png}
\caption{Mutual exclusion}
\label{img:mutex}
\end{figure}

\paragraph{Readers and writers}
Readers and writers, without any precedence, is show on figure~\ref{img:rw}. \emph{Double arrow} on\texttt{a} indicates that it can be run in parallel.

\underline{path} \{a\} + b \underline{end}

\begin{verbatim}
init(m) = 1, init(s) = 1
\end{verbatim}

\begin{Parallel}{}{}
\ParallelLText{
\begin{verbatim}
WW(s, c, m) {
  wait(m)
  c++
  if(c == 1)
    wait(s)
  signal(m)
}
\end{verbatim}}
\ParallelRText{
\begin{verbatim}
SS(s, c, m) {
  wait(m)
  c--
  if(c == 0)
    signal(s)
  signal(m)
}
\end{verbatim}}
\end{Parallel}

\begin{verbatim}
A: WW(s0, c, m) a SS(s1, c, m)
B: wait(s1) b signal(s0)
\end{verbatim}

\begin{figure}[hbtp]
\centering
\includegraphics[scale=0.6]{images/synchronization/pathexp_rw.png}
\caption{Readers and writers}
\label{img:rw}
\end{figure}

\paragraph{Example}
\underline{path} (a + b); (c + \{d\}); e \underline{end}

{\small
\begin{verbatim}
wait(s0) (a + b); (c + {d}); e signal(s0)
wait(s0) (a + b) signal(s1); wait(s1) (c + {d}); wait(s2) e signal(s0)

A: wait(s0) a signal(s1);
B: wait(s0) b signal(s1);
C: wait(s1) c signal(s2);
D: WW(s1, c, m) d SS(s2, c, m);
E: wait(s2) e signal(s0);
\end{verbatim}
\small}